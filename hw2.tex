%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For math
\usepackage{amsmath}
\usepackage{siunitx}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{6.867: Homework 2}

\begin{document}

\twocolumn[
  \icmltitle{6.867: Homework 2}

  % % It is OKAY to include author information, even for blind
  % % submissions: the style file will automatically remove it for you
  % % unless you've provided the [accepted] option to the icml2013
  % % package.
  % \icmlauthor{Your Name}{email@yourdomain.edu}
  % \icmladdress{Your Fantastic Institute,
  %             314159 Pi St., Palo Alto, CA 94306 USA}
  % \icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
  % \icmladdress{Their Fantastic Institute,
  %             27182 Exp St., Toronto, ON M6H 2T1 CANADA}

  % You may provide any keywords that you
  % find helpful for describing your paper; these are used to populate
  % the "keywords" metadata in the PDF but will not be shown in the document
  \icmlkeywords{boring formatting information, machine learning, ICML}

  \vskip 0.3in
]

\section{Logistic regression}
In this section, we explore logistic regression with L1 and L2 regularization. We use gradient descent to compare the resulting weight vectors under different regularizers and regularization parameters, and we evaluate the effect of these choices in the context of multiple data sets.

\subsection{L2 regularization}
We first consider L2 regularization, in which the objective function to minimize is
$$E_{LR}(w, w_0) = \textsc{NLL}(w, w_0) + \lambda |w|^2$$
where
$$\textsc{NLL}(w, w_0) = \sum_i \log(1 + \exp(-y^{(i)} (w x^{(i)} + w_0)))$$
and in the case of L2 regularization,
$$|w| = |w|_2 = \sqrt{w_1^2 + ... + w_n^2}$$
Gradient descent was run with this objective function on the training dataset \texttt{data1\_train.csv} with $\lambda=0$. Interestingly, as the number of gradient descent iterations, controlled by the step size and the convergence criterion, increased, the weight vector \_\_\_\_\_\_

\subsection{L1 regularization}
In the case of L1 regularization, we get that in the above equation,
$$|w| = |w|_1 = \sum_{i=1}^n |w_i|$$
We can evaluate the different regularization techniques under different values of $\lambda$ in the context of the weight vectors, the decision boundary, and the classification error rate in each of the training data sets.

\subsubsection{Weight vector}

\subsubsection{Decision boundary}

\subsubsection{Classification error rate}

\subsection{Optimization}
By using the training and validation data sets, we can identify the best regularizer and value for $\lambda$ for each of the four data sets. These results are presented in Table 1.
\begin{table}
  \begin{center}
    \begin{tabular}{ | c | c | c | c | }
      \hline
      Dataset & Best regularizer & Best $\lambda$ & Test performance \\ \hline
      1       & 5                & 6              & 1 \\ \hline
      2       & 5                & 6              & 1 \\ \hline
      3       & 5                & 6              & 1 \\ \hline
      4       & 5                & 6              & 1 \\ \hline
    \end{tabular}
  \end{center}
  \caption{Optimal regularizer and $\lambda$ for datasets}
\end{table}

\section{Support Vector Machine}
We here implement a dual form of linear SVMs with slack variables. More specifically, we solve the following optimization problem with respect to $\alpha$:
$$\max_\alpha -\frac{1}{2}|\sum_i \alpha_i y^{(i)} x^{(i)}|^2 + \sum_i \alpha_i$$
$$\text{s.t.} \sum_i \alpha_i y^{(i)} = 0$$
$$0 \leq \alpha_i \leq C, 1 \leq i \leq n$$
Written another way, this maximization problem can be framed as a minimiation problem:
$$\min_\alpha \frac{1}{2} x^T P x + q^T x$$
$$\text{s.t.} G x \leq h$$
$$ Ax = b $$
where $b=0$ and
$$x=
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \vdots      \\
  \alpha_n \\
\end{bmatrix},
q=
\begin{bmatrix}
  -1 \\
  -1 \\
  \vdots \\
  -1 \\
\end{bmatrix},
A^T=
\begin{bmatrix}
  y_0 \\
  y_1 \\
  \vdots \\
  y_{n-1} \\
\end{bmatrix},
G=
\begin{bmatrix}
  I \\
  -I \\
\end{bmatrix},
$$
where $I$ and $-I$ are the identity and negative identity matrix respectively. Furthermore,
$$
P=
\begin{bmatrix}
  x_0^2 y_0^2             & x_0 y_0 x_1 y_1         & \dots  & x_0 y_0 x_{n-1} y_{n-1}\\
  x_1 y_1 x_0 y_0         & x_1^2 y_1^2             & \dots  & x_1 y_1 x_{n-1} y_{n-1}\\
  \vdots                  & \vdots                  & \ddots & \vdots \\
  x_{n-1} y_{n-1} x_0 y_0 & x_{n-1} y_{n-1} x_1 y_1 & \dots  & x_{n-1}^2 y_{n-1}^2\\
\end{bmatrix}
$$
$$
h^T=
\begin{bmatrix}
  C & \dots & C & 0 & \dots & 0 \\
\end{bmatrix}
$$
In the context of the four-point 2D problem, we seek to solve the following optimization:
$$\min_\alpha \frac{1}{2}
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3 \\
\end{bmatrix}^T
\begin{bmatrix}
  16 & 24 & 0 & 24 \\
  24 & 36 & 0 & 36 \\
  0  & 0  & 0 & 0 \\
  24 & 36 & 0 & 36 \\
\end{bmatrix}
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3 \\
\end{bmatrix}
+
\begin{bmatrix}
  -1 \\
  -1 \\
  -1 \\
  -1 \\
\end{bmatrix}^T
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3 \\
\end{bmatrix}
$$
$$\text{s.t.}
\begin{bmatrix}
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 1 \\
  -1 & 0 & 0 & 0 \\
  0 & -1 & 0 & 0 \\
  0 & 0 & -1 & 0 \\
  0 & 0 & 0 & -1 \\
\end{bmatrix}
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3 \\
\end{bmatrix}
\leq
\begin{bmatrix}
  C \\
  C \\
  C \\
  C \\
  0 \\
  0 \\
  0 \\
  0 \\
\end{bmatrix},
\begin{bmatrix}
  2 \\
  3 \\
  -1 \\
  -2 \\
\end{bmatrix}
\begin{bmatrix}
  \alpha_0 \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3 \\
\end{bmatrix}
= 0 $$

Solving for $x$ when $C=1$, we get that $
\begin{bmatrix}
  \alpha_0 & \alpha_1 & \alpha_2 & \alpha_3
\end{bmatrix}^T
=
\begin{bmatrix}
  0.1875 & 0 & 0.3750 & 0 &
\end{bmatrix}^T
$. This indicates that the first and third samples are support vectors because $0 \leq \alpha_i \leq C$.



\section{Pegasos}

\section{Handwritten digit recognition}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
